{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUModel(\n",
      "  (gru1): LSTM(636228, 250, batch_first=True)\n",
      "  (fc): Linear(in_features=250, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Epoch [1/10], Loss: 0.5787402391433716, Training Accuracy: 50.00%, Validation Accuracy: 236.36%\n",
      "Epoch [2/10], Loss: 0.5879000425338745, Training Accuracy: 62.50%, Validation Accuracy: 236.36%\n",
      "Epoch [3/10], Loss: 0.7392970323562622, Training Accuracy: 55.00%, Validation Accuracy: 222.73%\n",
      "Epoch [4/10], Loss: 0.7620722055435181, Training Accuracy: 51.25%, Validation Accuracy: 245.45%\n",
      "Epoch [5/10], Loss: 0.7293870449066162, Training Accuracy: 53.75%, Validation Accuracy: 222.73%\n",
      "Epoch [6/10], Loss: 0.6814113855361938, Training Accuracy: 51.25%, Validation Accuracy: 222.73%\n",
      "Epoch [7/10], Loss: 0.7721406817436218, Training Accuracy: 51.25%, Validation Accuracy: 222.73%\n",
      "Epoch [8/10], Loss: 0.6910580396652222, Training Accuracy: 50.00%, Validation Accuracy: 222.73%\n",
      "Epoch [9/10], Loss: 0.6835445165634155, Training Accuracy: 51.25%, Validation Accuracy: 222.73%\n",
      "Epoch [10/10], Loss: 0.73436439037323, Training Accuracy: 51.25%, Validation Accuracy: 222.73%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru1 = nn.LSTM(input_size=input_size, hidden_size=250, batch_first=True, bidirectional=False)\n",
    "        self.fc = nn.Linear(250, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.gru1(x)\n",
    "        \n",
    "        if len(x.shape) == 3:\n",
    "            x = self.fc(x[:, -1, :])  # If the input has three dimensions, get the last output\n",
    "        elif len(x.shape) == 2:\n",
    "            x = self.fc(x.unsqueeze(1)).squeeze(1)  # If the input has two dimensions, add and squeeze a dimension\n",
    "        else:\n",
    "            raise ValueError(\"Input tensor must have either two or three dimensions.\")\n",
    "        \n",
    "        x = self.sigmoid(x)  # Adjust to have the same size as the target\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Input shape\n",
    "input_shape = (80, 636228, 1)\n",
    "\n",
    "# Create the PyTorch model\n",
    "model = GRUModel(input_shape[1])\n",
    "print(model)\n",
    "\n",
    "file_path = 'Padded_Training.npz'\n",
    "npz_file = np.load(file_path)\n",
    "\n",
    "# Access individual arrays\n",
    "TrainF = npz_file['features']\n",
    "TrainL = npz_file['labels']\n",
    "\n",
    "X_train_tensor = torch.tensor(TrainF, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(TrainL, dtype=torch.float32)  # Change dtype to float32 for binary classification\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
    "\n",
    "# Validation data\n",
    "val_file_path = 'Padded_Testing.npz'\n",
    "val_npz_file = np.load(val_file_path)\n",
    "ValF = val_npz_file['features']\n",
    "ValL = val_npz_file['labels']\n",
    "X_val_tensor = torch.tensor(ValF, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(ValL, dtype=torch.float32)  # Change dtype to float32 for binary classification\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=5, shuffle=False) \n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # Change loss function for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training, Validation, and Testing loops\n",
    "num_epochs = 10\n",
    "\n",
    "# ...\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    total_correct_train = 0\n",
    "    total_samples_train = 0\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        outputs = model(batch_X).squeeze(dim=1)\n",
    "\n",
    "        # Ensure the target tensor is in the range [0, 1]\n",
    "        batch_y = batch_y.float()  # Convert to float\n",
    "        #batch_y = batch_y.unsqueeze(1)  # Ensure it has the correct shape\n",
    "\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        predicted_train = (outputs > 0.5).float()\n",
    "        total_correct_train += (predicted_train == batch_y).sum().item()\n",
    "        total_samples_train += batch_y.size(0)\n",
    "\n",
    "    accuracy_train = total_correct_train / total_samples_train\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    total_correct_val = 0\n",
    "    total_samples_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X_val, batch_y_val in val_loader:\n",
    "            outputs_val = model(batch_X_val).squeeze(dim=1)\n",
    "\n",
    "            # Ensure the target tensor is in the range [0, 1]\n",
    "            batch_y_val = batch_y_val.float()  # Convert to float\n",
    "            batch_y_val = batch_y_val.unsqueeze(1)  # Ensure it has the correct shape\n",
    "\n",
    "            predicted_val = (outputs_val > 0.5).float()\n",
    "            total_correct_val += (predicted_val == batch_y_val).sum().item()\n",
    "            total_samples_val += batch_y_val.size(0)\n",
    "\n",
    "    accuracy_val = total_correct_val / total_samples_val\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}, '\n",
    "          f'Training Accuracy: {accuracy_train * 100:.2f}%, '\n",
    "          f'Validation Accuracy: {accuracy_val * 100:.2f}%')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUModel(\n",
      "  (gru1): LSTM(636228, 250, batch_first=True, bidirectional=True)\n",
      "  (fc): Linear(in_features=250, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (5x500 and 250x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 76\u001b[0m\n\u001b[0;32m     73\u001b[0m total_samples_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_X, batch_y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m---> 76\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;66;03m# Ensure the target tensor is in the range [0, 1]\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     batch_y \u001b[38;5;241m=\u001b[39m batch_y\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# Convert to float\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 20\u001b[0m, in \u001b[0;36mGRUModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     18\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])  \u001b[38;5;66;03m# If the input has three dimensions, get the last output\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m---> 20\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# If the input has two dimensions, add and squeeze a dimension\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput tensor must have either two or three dimensions.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (5x500 and 250x1)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru1 = nn.LSTM(input_size=input_size, hidden_size=250, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(250, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.gru1(x)\n",
    "        \n",
    "        if len(x.shape) == 3:\n",
    "            x = self.fc(x[:, -1, :])  # If the input has three dimensions, get the last output\n",
    "        elif len(x.shape) == 2:\n",
    "            x = self.fc(x.unsqueeze(1)).squeeze(1)  # If the input has two dimensions, add and squeeze a dimension\n",
    "        else:\n",
    "            raise ValueError(\"Input tensor must have either two or three dimensions.\")\n",
    "        \n",
    "        x = self.sigmoid(x)  # Adjust to have the same size as the target\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Input shape\n",
    "input_shape = (80, 636228, 1)\n",
    "\n",
    "# Create the PyTorch model\n",
    "model = GRUModel(input_shape[1])\n",
    "print(model)\n",
    "\n",
    "file_path = 'Padded_Training.npz'\n",
    "npz_file = np.load(file_path)\n",
    "\n",
    "# Access individual arrays\n",
    "TrainF = npz_file['features']\n",
    "TrainL = npz_file['labels']\n",
    "\n",
    "X_train_tensor = torch.tensor(TrainF, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(TrainL, dtype=torch.float32)  # Change dtype to float32 for binary classification\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
    "\n",
    "# Validation data\n",
    "val_file_path = 'Padded_Testing.npz'\n",
    "val_npz_file = np.load(val_file_path)\n",
    "ValF = val_npz_file['features']\n",
    "ValL = val_npz_file['labels']\n",
    "X_val_tensor = torch.tensor(ValF, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(ValL, dtype=torch.float32)  # Change dtype to float32 for binary classification\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=5, shuffle=False) \n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # Change loss function for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training, Validation, and Testing loops\n",
    "num_epochs = 10\n",
    "\n",
    "# ...\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    total_correct_train = 0\n",
    "    total_samples_train = 0\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        outputs = model(batch_X).squeeze(dim=1)\n",
    "\n",
    "        # Ensure the target tensor is in the range [0, 1]\n",
    "        batch_y = batch_y.float()  # Convert to float\n",
    "        #batch_y = batch_y.unsqueeze(1)  # Ensure it has the correct shape\n",
    "\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        predicted_train = (outputs > 0.5).float()\n",
    "        total_correct_train += (predicted_train == batch_y).sum().item()\n",
    "        total_samples_train += batch_y.size(0)\n",
    "\n",
    "    accuracy_train = total_correct_train / total_samples_train\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    total_correct_val = 0\n",
    "    total_samples_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X_val, batch_y_val in val_loader:\n",
    "            outputs_val = model(batch_X_val).squeeze(dim=1)\n",
    "\n",
    "            # Ensure the target tensor is in the range [0, 1]\n",
    "            batch_y_val = batch_y_val.float()  # Convert to float\n",
    "            batch_y_val = batch_y_val.unsqueeze(1)  # Ensure it has the correct shape\n",
    "\n",
    "            predicted_val = (outputs_val > 0.5).float()\n",
    "            total_correct_val += (predicted_val == batch_y_val).sum().item()\n",
    "            total_samples_val += batch_y_val.size(0)\n",
    "\n",
    "    accuracy_val = total_correct_val / total_samples_val\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}, '\n",
    "          f'Training Accuracy: {accuracy_train * 100:.2f}%, '\n",
    "          f'Validation Accuracy: {accuracy_val * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GRU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUModel(\n",
      "  (gru1): GRU(636228, 250, batch_first=True)\n",
      "  (fc): Linear(in_features=250, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Epoch [1/10], Loss: 0.5585112571716309, Training Accuracy: 46.25%, Validation Accuracy: 268.18%\n",
      "Epoch [2/10], Loss: 0.7615493535995483, Training Accuracy: 52.50%, Validation Accuracy: 222.73%\n",
      "Epoch [3/10], Loss: 0.6699438095092773, Training Accuracy: 50.00%, Validation Accuracy: 222.73%\n",
      "Epoch [4/10], Loss: 0.7853032350540161, Training Accuracy: 50.00%, Validation Accuracy: 222.73%\n",
      "Epoch [5/10], Loss: 0.6887288093566895, Training Accuracy: 50.00%, Validation Accuracy: 222.73%\n",
      "Epoch [6/10], Loss: 0.7137421369552612, Training Accuracy: 50.00%, Validation Accuracy: 222.73%\n",
      "Epoch [7/10], Loss: 0.6767549514770508, Training Accuracy: 50.00%, Validation Accuracy: 222.73%\n",
      "Epoch [8/10], Loss: 0.6922649145126343, Training Accuracy: 50.00%, Validation Accuracy: 222.73%\n",
      "Epoch [9/10], Loss: 0.7265862226486206, Training Accuracy: 51.25%, Validation Accuracy: 222.73%\n",
      "Epoch [10/10], Loss: 0.6863322854042053, Training Accuracy: 50.00%, Validation Accuracy: 222.73%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru1 = nn.GRU(input_size=input_size, hidden_size=250, batch_first=True, bidirectional=False)\n",
    "        self.fc = nn.Linear(250, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.gru1(x)\n",
    "        \n",
    "        if len(x.shape) == 3:\n",
    "            x = self.fc(x[:, -1, :])  # If the input has three dimensions, get the last output\n",
    "        elif len(x.shape) == 2:\n",
    "            x = self.fc(x.unsqueeze(1)).squeeze(1)  # If the input has two dimensions, add and squeeze a dimension\n",
    "        else:\n",
    "            raise ValueError(\"Input tensor must have either two or three dimensions.\")\n",
    "        \n",
    "        x = self.sigmoid(x)  # Adjust to have the same size as the target\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Input shape\n",
    "input_shape = (80, 636228, 1)\n",
    "\n",
    "# Create the PyTorch model\n",
    "model = GRUModel(input_shape[1])\n",
    "print(model)\n",
    "\n",
    "file_path = 'Padded_Training.npz'\n",
    "npz_file = np.load(file_path)\n",
    "\n",
    "# Access individual arrays\n",
    "TrainF = npz_file['features']\n",
    "TrainL = npz_file['labels']\n",
    "\n",
    "X_train_tensor = torch.tensor(TrainF, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(TrainL, dtype=torch.float32)  # Change dtype to float32 for binary classification\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
    "\n",
    "# Validation data\n",
    "val_file_path = 'Padded_Testing.npz'\n",
    "val_npz_file = np.load(val_file_path)\n",
    "ValF = val_npz_file['features']\n",
    "ValL = val_npz_file['labels']\n",
    "X_val_tensor = torch.tensor(ValF, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(ValL, dtype=torch.float32)  # Change dtype to float32 for binary classification\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=5, shuffle=False) \n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # Change loss function for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training, Validation, and Testing loops\n",
    "num_epochs = 10\n",
    "\n",
    "# ...\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    total_correct_train = 0\n",
    "    total_samples_train = 0\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        outputs = model(batch_X).squeeze(dim=1)\n",
    "\n",
    "        # Ensure the target tensor is in the range [0, 1]\n",
    "        batch_y = batch_y.float()  # Convert to float\n",
    "        #batch_y = batch_y.unsqueeze(1)  # Ensure it has the correct shape\n",
    "\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        predicted_train = (outputs > 0.5).float()\n",
    "        total_correct_train += (predicted_train == batch_y).sum().item()\n",
    "        total_samples_train += batch_y.size(0)\n",
    "\n",
    "    accuracy_train = total_correct_train / total_samples_train\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    total_correct_val = 0\n",
    "    total_samples_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X_val, batch_y_val in val_loader:\n",
    "            outputs_val = model(batch_X_val).squeeze(dim=1)\n",
    "\n",
    "            # Ensure the target tensor is in the range [0, 1]\n",
    "            batch_y_val = batch_y_val.float()  # Convert to float\n",
    "            batch_y_val = batch_y_val.unsqueeze(1)  # Ensure it has the correct shape\n",
    "\n",
    "            predicted_val = (outputs_val > 0.5).float()\n",
    "            total_correct_val += (predicted_val == batch_y_val).sum().item()\n",
    "            total_samples_val += batch_y_val.size(0)\n",
    "\n",
    "    accuracy_val = total_correct_val / total_samples_val\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}, '\n",
    "          f'Training Accuracy: {accuracy_train * 100:.2f}%, '\n",
    "          f'Validation Accuracy: {accuracy_val * 100:.2f}%')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMModel(\n",
      "  (lstm1): LSTM(636228, 250, batch_first=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Epoch [1/10], Loss: 5.507979393005371, Training Accuracy: 0.00%, Validation Accuracy: 0.00%\n",
      "Epoch [2/10], Loss: 5.508194923400879, Training Accuracy: 0.00%, Validation Accuracy: 0.00%\n",
      "Epoch [3/10], Loss: 5.506131172180176, Training Accuracy: 0.00%, Validation Accuracy: 0.00%\n",
      "Epoch [4/10], Loss: 5.509807109832764, Training Accuracy: 0.00%, Validation Accuracy: 0.00%\n",
      "Epoch [5/10], Loss: 5.506494998931885, Training Accuracy: 0.00%, Validation Accuracy: 0.00%\n",
      "Epoch [6/10], Loss: 5.48881721496582, Training Accuracy: 0.00%, Validation Accuracy: 0.00%\n",
      "Epoch [7/10], Loss: 5.486522197723389, Training Accuracy: 0.00%, Validation Accuracy: 0.00%\n",
      "Epoch [8/10], Loss: 5.494141101837158, Training Accuracy: 0.00%, Validation Accuracy: 0.00%\n",
      "Epoch [9/10], Loss: 5.5052571296691895, Training Accuracy: 2.50%, Validation Accuracy: 0.00%\n",
      "Epoch [10/10], Loss: 5.419958114624023, Training Accuracy: 5.00%, Validation Accuracy: 9.09%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=250, batch_first=True, bidirectional=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Input shape\n",
    "input_shape = (80, 636228, 1)\n",
    "num_classes = 2\n",
    "\n",
    "# Create the PyTorch model\n",
    "model = LSTMModel(input_shape[1], 2)\n",
    "print(model)\n",
    "\n",
    "file_path = 'Padded_Training.npz'\n",
    "npz_file = np.load(file_path)\n",
    "\n",
    "# Access individual arrays\n",
    "TrainF = npz_file['features']\n",
    "TrainL = npz_file['labels']\n",
    "\n",
    "X_train_tensor = torch.tensor(TrainF, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(TrainL, dtype=torch.long)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "\n",
    "# Validation data\n",
    "val_file_path = 'Padded_Testing.npz'\n",
    "val_npz_file = np.load(val_file_path)\n",
    "ValF = val_npz_file['features']\n",
    "ValL = val_npz_file['labels']\n",
    "X_val_tensor = torch.tensor(ValF, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(ValL, dtype=torch.long)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training, Validation, and Testing loops\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    total_correct_train = 0\n",
    "    total_samples_train = 0\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted_train = torch.max(outputs, 1)\n",
    "        total_correct_train += (predicted_train == batch_y).sum().item()\n",
    "        total_samples_train += batch_y.size(0)\n",
    "\n",
    "    accuracy_train = total_correct_train / total_samples_train\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    total_correct_val = 0\n",
    "    total_samples_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X_val, batch_y_val in val_loader:\n",
    "            outputs_val = model(batch_X_val)\n",
    "            _, predicted_val = torch.max(outputs_val, 1)\n",
    "            total_correct_val += (predicted_val == batch_y_val).sum().item()\n",
    "            total_samples_val += batch_y_val.size(0)\n",
    "\n",
    "    accuracy_val = total_correct_val / total_samples_val\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}, '\n",
    "          f'Training Accuracy: {accuracy_train * 100:.2f}%, '\n",
    "          f'Validation Accuracy: {accuracy_val * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 68.18%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "svm_model = SVC(kernel='rbf', C=10)\n",
    "\n",
    "# Train the model on the training data\n",
    "svm_model.fit(TrainF, TrainL)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = svm_model.predict(ValF)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(ValL, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "nb_model = GaussianNB()\n",
    "\n",
    "nb_model.fit(TrainF, TrainL)\n",
    "\n",
    "y_pred = nb_model.predict(ValF)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(ValL, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 54.55%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming you already have TrainF, TrainL, ValF, and ValL\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=50)\n",
    "\n",
    "# Train the model on the training data\n",
    "rf_model.fit(TrainF, TrainL)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_rf = rf_model.predict(ValF)\n",
    "\n",
    "# Evaluate the accuracy of the Random Forest model\n",
    "accuracy_rf = accuracy_score(ValL, y_pred_rf)\n",
    "print(f\"Random Forest Accuracy: {accuracy_rf * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
