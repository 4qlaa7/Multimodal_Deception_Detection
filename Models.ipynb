{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GRU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUModel(\n",
      "  (gru1): GRU(636228, 250, batch_first=True)\n",
      "  (drop1): Dropout(p=0.02, inplace=False)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Epoch [1/10], Loss: 5.645654678344727, Training Accuracy: 22.50%, Validation Accuracy: 9.09%\n",
      "Epoch [2/10], Loss: 5.600015163421631, Training Accuracy: 25.00%, Validation Accuracy: 9.09%\n",
      "Epoch [3/10], Loss: 5.537501335144043, Training Accuracy: 30.00%, Validation Accuracy: 9.09%\n",
      "Epoch [4/10], Loss: 5.525794982910156, Training Accuracy: 12.50%, Validation Accuracy: 9.09%\n",
      "Epoch [5/10], Loss: 5.519596099853516, Training Accuracy: 17.50%, Validation Accuracy: 9.09%\n",
      "Epoch [6/10], Loss: 5.59866189956665, Training Accuracy: 20.00%, Validation Accuracy: 9.09%\n",
      "Epoch [7/10], Loss: 5.586488246917725, Training Accuracy: 7.50%, Validation Accuracy: 9.09%\n",
      "Epoch [8/10], Loss: 5.628485679626465, Training Accuracy: 11.25%, Validation Accuracy: 9.09%\n",
      "Epoch [9/10], Loss: 5.526776313781738, Training Accuracy: 15.00%, Validation Accuracy: 9.09%\n",
      "Epoch [10/10], Loss: 5.620068073272705, Training Accuracy: 11.25%, Validation Accuracy: 9.09%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru1 = nn.GRU(input_size=input_size, hidden_size=250, batch_first=True, bidirectional=False)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Transpose input tensor to have the correct dimensions\n",
    "        #x = x.permute(0, 2, 1)\n",
    "        \n",
    "        x, _ = self.gru1(x)\n",
    "        \n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Input shape\n",
    "input_shape = (80, 636228,1)\n",
    "num_classes = 2\n",
    "\n",
    "# Create the PyTorch model\n",
    "model = GRUModel(input_shape[1], 2)\n",
    "print(model)\n",
    "\n",
    "file_path = 'Padded_Training.npz'\n",
    "npz_file = np.load(file_path)\n",
    "\n",
    "# Access individual arrays\n",
    "TrainF = npz_file['features']\n",
    "TrainL = npz_file['labels']\n",
    "\n",
    "X_train_tensor = torch.tensor(TrainF, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(TrainL, dtype=torch.long)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "\n",
    "\n",
    "# Validation data\n",
    "val_file_path = 'Padded_Testing.npz'\n",
    "val_npz_file = np.load(val_file_path)\n",
    "ValF = val_npz_file['features']\n",
    "ValL = val_npz_file['labels']\n",
    "X_val_tensor = torch.tensor(ValF, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(ValL, dtype=torch.long)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False) \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training, Validation, and Testing loops\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    total_correct_train = 0\n",
    "    total_samples_train = 0\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted_train = torch.max(outputs, 1)\n",
    "        total_correct_train += (predicted_train == batch_y).sum().item()\n",
    "        total_samples_train += batch_y.size(0)\n",
    "\n",
    "    accuracy_train = total_correct_train / total_samples_train\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    total_correct_val = 0\n",
    "    total_samples_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X_val, batch_y_val in val_loader:\n",
    "            outputs_val = model(batch_X_val)\n",
    "            _, predicted_val = torch.max(outputs_val, 1)\n",
    "            total_correct_val += (predicted_val == batch_y_val).sum().item()\n",
    "            total_samples_val += batch_y_val.size(0)\n",
    "\n",
    "    accuracy_val = total_correct_val / total_samples_val\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}, '\n",
    "          f'Training Accuracy: {accuracy_train * 100:.2f}%, '\n",
    "          f'Validation Accuracy: {accuracy_val * 100:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.255573\n"
     ]
    }
   ],
   "source": [
    "print(TrainF[0][70000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMModel(\n",
      "  (lstm1): LSTM(636228, 250, batch_first=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Epoch [1/10], Loss: 5.507979393005371, Training Accuracy: 0.00%, Validation Accuracy: 0.00%\n",
      "Epoch [2/10], Loss: 5.508194923400879, Training Accuracy: 0.00%, Validation Accuracy: 0.00%\n",
      "Epoch [3/10], Loss: 5.506131172180176, Training Accuracy: 0.00%, Validation Accuracy: 0.00%\n",
      "Epoch [4/10], Loss: 5.509807109832764, Training Accuracy: 0.00%, Validation Accuracy: 0.00%\n",
      "Epoch [5/10], Loss: 5.506494998931885, Training Accuracy: 0.00%, Validation Accuracy: 0.00%\n",
      "Epoch [6/10], Loss: 5.48881721496582, Training Accuracy: 0.00%, Validation Accuracy: 0.00%\n",
      "Epoch [7/10], Loss: 5.486522197723389, Training Accuracy: 0.00%, Validation Accuracy: 0.00%\n",
      "Epoch [8/10], Loss: 5.494141101837158, Training Accuracy: 0.00%, Validation Accuracy: 0.00%\n",
      "Epoch [9/10], Loss: 5.5052571296691895, Training Accuracy: 2.50%, Validation Accuracy: 0.00%\n",
      "Epoch [10/10], Loss: 5.419958114624023, Training Accuracy: 5.00%, Validation Accuracy: 9.09%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=250, batch_first=True, bidirectional=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Input shape\n",
    "input_shape = (80, 636228, 1)\n",
    "num_classes = 2\n",
    "\n",
    "# Create the PyTorch model\n",
    "model = LSTMModel(input_shape[1], 2)\n",
    "print(model)\n",
    "\n",
    "file_path = 'Padded_Training.npz'\n",
    "npz_file = np.load(file_path)\n",
    "\n",
    "# Access individual arrays\n",
    "TrainF = npz_file['features']\n",
    "TrainL = npz_file['labels']\n",
    "\n",
    "X_train_tensor = torch.tensor(TrainF, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(TrainL, dtype=torch.long)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "\n",
    "# Validation data\n",
    "val_file_path = 'Padded_Testing.npz'\n",
    "val_npz_file = np.load(val_file_path)\n",
    "ValF = val_npz_file['features']\n",
    "ValL = val_npz_file['labels']\n",
    "X_val_tensor = torch.tensor(ValF, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(ValL, dtype=torch.long)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training, Validation, and Testing loops\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    total_correct_train = 0\n",
    "    total_samples_train = 0\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted_train = torch.max(outputs, 1)\n",
    "        total_correct_train += (predicted_train == batch_y).sum().item()\n",
    "        total_samples_train += batch_y.size(0)\n",
    "\n",
    "    accuracy_train = total_correct_train / total_samples_train\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    total_correct_val = 0\n",
    "    total_samples_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X_val, batch_y_val in val_loader:\n",
    "            outputs_val = model(batch_X_val)\n",
    "            _, predicted_val = torch.max(outputs_val, 1)\n",
    "            total_correct_val += (predicted_val == batch_y_val).sum().item()\n",
    "            total_samples_val += batch_y_val.size(0)\n",
    "\n",
    "    accuracy_val = total_correct_val / total_samples_val\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}, '\n",
    "          f'Training Accuracy: {accuracy_train * 100:.2f}%, '\n",
    "          f'Validation Accuracy: {accuracy_val * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 68.18%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "svm_model = SVC(kernel='rbf', C=10)\n",
    "\n",
    "# Train the model on the training data\n",
    "svm_model.fit(TrainF, TrainL)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = svm_model.predict(ValF)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(ValL, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "nb_model = GaussianNB()\n",
    "\n",
    "nb_model.fit(TrainF, TrainL)\n",
    "\n",
    "y_pred = nb_model.predict(ValF)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(ValL, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 54.55%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming you already have TrainF, TrainL, ValF, and ValL\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=50)\n",
    "\n",
    "# Train the model on the training data\n",
    "rf_model.fit(TrainF, TrainL)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_rf = rf_model.predict(ValF)\n",
    "\n",
    "# Evaluate the accuracy of the Random Forest model\n",
    "accuracy_rf = accuracy_score(ValL, y_pred_rf)\n",
    "print(f\"Random Forest Accuracy: {accuracy_rf * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
