{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUModel(\n",
      "  (gru1): LSTM(636228, 250, batch_first=True)\n",
      "  (fc): Linear(in_features=250, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Epoch [1/10], Average Loss: 0.7154402919113636, Training Accuracy: 51.25%, Validation Accuracy: 36.36%\n",
      "Epoch [2/10], Average Loss: 0.7033217065036297, Training Accuracy: 47.50%, Validation Accuracy: 45.45%\n",
      "Epoch [3/10], Average Loss: 0.696284182369709, Training Accuracy: 48.75%, Validation Accuracy: 45.45%\n",
      "Epoch [4/10], Average Loss: 0.6923133693635464, Training Accuracy: 50.00%, Validation Accuracy: 45.45%\n",
      "Epoch [5/10], Average Loss: 0.689653780311346, Training Accuracy: 50.00%, Validation Accuracy: 45.45%\n",
      "Epoch [6/10], Average Loss: 0.6891142316162586, Training Accuracy: 50.00%, Validation Accuracy: 45.45%\n",
      "Epoch [7/10], Average Loss: 0.6927569024264812, Training Accuracy: 50.00%, Validation Accuracy: 45.45%\n",
      "Epoch [8/10], Average Loss: 0.6898260712623596, Training Accuracy: 50.00%, Validation Accuracy: 40.91%\n",
      "Epoch [9/10], Average Loss: 0.6914833299815655, Training Accuracy: 50.00%, Validation Accuracy: 40.91%\n",
      "Epoch [10/10], Average Loss: 0.6901916302740574, Training Accuracy: 50.00%, Validation Accuracy: 40.91%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.gru1 = nn.LSTM(input_size=input_size, hidden_size=250, batch_first=True, bidirectional=False)\n",
    "        self.fc = nn.Linear(250, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.gru1(x)\n",
    "        \n",
    "        if len(x.shape) == 3:\n",
    "            x = self.fc(x[:, -1, :])\n",
    "        elif len(x.shape) == 2:\n",
    "            x = self.fc(x.unsqueeze(1)).squeeze(1)\n",
    "        else:\n",
    "            raise ValueError(\"Input tensor must have either two or three dimensions.\")\n",
    "        \n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Function to initialize model weights\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "# Input shape\n",
    "input_shape = (80, 636228, 1)\n",
    "\n",
    "# Create the PyTorch model with weight initialization\n",
    "model = GRUModel(input_shape[1])\n",
    "model.apply(init_weights)\n",
    "print(model)\n",
    "\n",
    "file_path = 'Padded_Training.npz'\n",
    "npz_file = np.load(file_path)\n",
    "\n",
    "# Access individual arrays\n",
    "TrainF = npz_file['features']\n",
    "TrainL = npz_file['labels']\n",
    "\n",
    "X_train_tensor = torch.tensor(TrainF, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(TrainL, dtype=torch.float32)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
    "\n",
    "# Validation data\n",
    "val_file_path = 'Padded_Testing.npz'\n",
    "val_npz_file = np.load(val_file_path)\n",
    "ValF = val_npz_file['features']\n",
    "ValL = val_npz_file['labels']\n",
    "X_val_tensor = torch.tensor(ValF, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(ValL, dtype=torch.float32)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=5, shuffle=False)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# Training, Validation, and Testing loops\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    total_loss_train = 0\n",
    "    total_correct_train = 0\n",
    "    total_samples_train = 0\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        outputs = model(batch_X).squeeze(dim=1)\n",
    "        batch_y = batch_y.float()\n",
    "\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss_train += loss.item()\n",
    "\n",
    "        predicted_train = (outputs > 0.5).float()\n",
    "        total_correct_train += (predicted_train == batch_y).sum().item()\n",
    "        total_samples_train += batch_y.size(0)\n",
    "\n",
    "    # Calculate average training loss for the epoch\n",
    "    average_loss_train = total_loss_train / len(train_loader)\n",
    "    accuracy_train = total_correct_train / total_samples_train\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    total_correct_val = 0\n",
    "    total_samples_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X_val, batch_y_val in val_loader:\n",
    "            outputs_val = model(batch_X_val).squeeze(dim=1)\n",
    "            batch_y_val = batch_y_val.float()\n",
    "\n",
    "            predicted_val = (outputs_val > 0.5).float()\n",
    "            total_correct_val += (predicted_val == batch_y_val).sum().item()\n",
    "            total_samples_val += batch_y_val.size(0)\n",
    "\n",
    "    accuracy_val = total_correct_val / total_samples_val\n",
    "\n",
    "    # Print metrics for the epoch\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Average Loss: {average_loss_train}, '\n",
    "          f'Training Accuracy: {accuracy_train * 100:.2f}%, '\n",
    "          f'Validation Accuracy: {accuracy_val * 100:.2f}%')\n",
    "\n",
    "    # Adjust learning rate using scheduler\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GRU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUModel(\n",
      "  (gru1): GRU(636228, 250, batch_first=True)\n",
      "  (fc): Linear(in_features=250, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Epoch [1/10], Loss: 0.7357641458511353, Training Accuracy: 45.00%, Validation Accuracy: 222.73%\n",
      "Epoch [2/10], Loss: 0.8199621438980103, Training Accuracy: 50.00%, Validation Accuracy: 222.73%\n",
      "Epoch [3/10], Loss: 0.6849316954612732, Training Accuracy: 50.00%, Validation Accuracy: 222.73%\n",
      "Epoch [4/10], Loss: 0.7496523261070251, Training Accuracy: 50.00%, Validation Accuracy: 222.73%\n",
      "Epoch [5/10], Loss: 0.6766282320022583, Training Accuracy: 50.00%, Validation Accuracy: 222.73%\n",
      "Epoch [6/10], Loss: 0.6851674318313599, Training Accuracy: 50.00%, Validation Accuracy: 222.73%\n",
      "Epoch [7/10], Loss: 0.6269125938415527, Training Accuracy: 50.00%, Validation Accuracy: 222.73%\n",
      "Epoch [8/10], Loss: 0.7277967929840088, Training Accuracy: 50.00%, Validation Accuracy: 222.73%\n",
      "Epoch [9/10], Loss: 0.7023771405220032, Training Accuracy: 50.00%, Validation Accuracy: 222.73%\n",
      "Epoch [10/10], Loss: 0.7348083853721619, Training Accuracy: 50.00%, Validation Accuracy: 222.73%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru1 = nn.GRU(input_size=input_size, hidden_size=250, batch_first=True, bidirectional=False)\n",
    "        self.fc = nn.Linear(250, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.gru1(x)\n",
    "        \n",
    "        if len(x.shape) == 3:\n",
    "            x = self.fc(x[:, -1, :])  # If the input has three dimensions, get the last output\n",
    "        elif len(x.shape) == 2:\n",
    "            x = self.fc(x.unsqueeze(1)).squeeze(1)  # If the input has two dimensions, add and squeeze a dimension\n",
    "        else:\n",
    "            raise ValueError(\"Input tensor must have either two or three dimensions.\")\n",
    "        \n",
    "        x = self.sigmoid(x)  # Adjust to have the same size as the target\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_shape = (80, 636228, 1)\n",
    "\n",
    "\n",
    "model = GRUModel(input_shape[1])\n",
    "print(model)\n",
    "\n",
    "file_path = 'Padded_Training.npz'\n",
    "npz_file = np.load(file_path)\n",
    "\n",
    "# Access individual arrays\n",
    "TrainF = npz_file['features']\n",
    "TrainL = npz_file['labels']\n",
    "\n",
    "X_train_tensor = torch.tensor(TrainF, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(TrainL, dtype=torch.float32)  # Change dtype to float32 for binary classification\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
    "\n",
    "# Validation data\n",
    "val_file_path = 'Padded_Testing.npz'\n",
    "val_npz_file = np.load(val_file_path)\n",
    "ValF = val_npz_file['features']\n",
    "ValL = val_npz_file['labels']\n",
    "X_val_tensor = torch.tensor(ValF, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(ValL, dtype=torch.float32)  # Change dtype to float32 for binary classification\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=5, shuffle=False) \n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # Change loss function for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training, Validation, and Testing loops\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    total_correct_train = 0\n",
    "    total_samples_train = 0\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        outputs = model(batch_X).squeeze(dim=1)\n",
    "\n",
    "        # Ensure the target tensor is in the range [0, 1]\n",
    "        batch_y = batch_y.float()  # Convert to float\n",
    "        #batch_y = batch_y.unsqueeze(1)  # Ensure it has the correct shape\n",
    "\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        predicted_train = (outputs > 0.5).float()\n",
    "        total_correct_train += (predicted_train == batch_y).sum().item()\n",
    "        total_samples_train += batch_y.size(0)\n",
    "\n",
    "    accuracy_train = total_correct_train / total_samples_train\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    total_correct_val = 0\n",
    "    total_samples_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X_val, batch_y_val in val_loader:\n",
    "            outputs_val = model(batch_X_val).squeeze(dim=1)\n",
    "\n",
    "            # Ensure the target tensor is in the range [0, 1]\n",
    "            batch_y_val = batch_y_val.float()  # Convert to float\n",
    "            batch_y_val = batch_y_val.unsqueeze(1)  # Ensure it has the correct shape\n",
    "\n",
    "            predicted_val = (outputs_val > 0.5).float()\n",
    "            total_correct_val += (predicted_val == batch_y_val).sum().item()\n",
    "            total_samples_val += batch_y_val.size(0)\n",
    "\n",
    "    accuracy_val = total_correct_val / total_samples_val\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}, '\n",
    "          f'Training Accuracy: {accuracy_train * 100:.2f}%, '\n",
    "          f'Validation Accuracy: {accuracy_val * 100:.2f}%')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUModel(\n",
      "  (gru1): GRU(636228, 250, batch_first=True)\n",
      "  (fc): Linear(in_features=250, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Epoch [1/10], Average Loss: 0.7094924040138721, Training Accuracy: 48.75%, Validation Accuracy: 45.45%\n",
      "Epoch [2/10], Average Loss: 0.7028701528906822, Training Accuracy: 48.75%, Validation Accuracy: 50.00%\n",
      "Epoch [3/10], Average Loss: 0.6911830827593803, Training Accuracy: 51.25%, Validation Accuracy: 54.55%\n",
      "Epoch [4/10], Average Loss: 0.6893772594630718, Training Accuracy: 52.50%, Validation Accuracy: 54.55%\n",
      "Epoch [5/10], Average Loss: 0.70161347463727, Training Accuracy: 48.75%, Validation Accuracy: 54.55%\n",
      "Epoch [6/10], Average Loss: 0.6885979510843754, Training Accuracy: 50.00%, Validation Accuracy: 54.55%\n",
      "Epoch [7/10], Average Loss: 0.6973218694329262, Training Accuracy: 50.00%, Validation Accuracy: 54.55%\n",
      "Epoch [8/10], Average Loss: 0.6945449151098728, Training Accuracy: 51.25%, Validation Accuracy: 54.55%\n",
      "Epoch [9/10], Average Loss: 0.6900071986019611, Training Accuracy: 51.25%, Validation Accuracy: 54.55%\n",
      "Epoch [10/10], Average Loss: 0.692377369850874, Training Accuracy: 50.00%, Validation Accuracy: 54.55%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru1 = nn.GRU(input_size=input_size, hidden_size=250, batch_first=True, bidirectional=False)\n",
    "        self.fc = nn.Linear(250, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.gru1(x)\n",
    "        \n",
    "        if len(x.shape) == 3:\n",
    "            x = self.fc(x[:, -1, :])\n",
    "        elif len(x.shape) == 2:\n",
    "            x = self.fc(x.unsqueeze(1)).squeeze(1)\n",
    "        else:\n",
    "            raise ValueError(\"Input tensor must have either two or three dimensions.\")\n",
    "        \n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Function to initialize model weights\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "# Input shape\n",
    "input_shape = (80, 636228, 1)\n",
    "\n",
    "# Create the PyTorch model with weight initialization\n",
    "model = GRUModel(input_shape[1])\n",
    "model.apply(init_weights)\n",
    "print(model)\n",
    "\n",
    "file_path = 'Padded_Training.npz'\n",
    "npz_file = np.load(file_path)\n",
    "\n",
    "# Access individual arrays\n",
    "TrainF = npz_file['features']\n",
    "TrainL = npz_file['labels']\n",
    "\n",
    "X_train_tensor = torch.tensor(TrainF, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(TrainL, dtype=torch.float32)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
    "\n",
    "# Validation data\n",
    "val_file_path = 'Padded_Testing.npz'\n",
    "val_npz_file = np.load(val_file_path)\n",
    "ValF = val_npz_file['features']\n",
    "ValL = val_npz_file['labels']\n",
    "X_val_tensor = torch.tensor(ValF, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(ValL, dtype=torch.float32)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=5, shuffle=False)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# Training, Validation, and Testing loops\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    total_loss_train = 0\n",
    "    total_correct_train = 0\n",
    "    total_samples_train = 0\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        outputs = model(batch_X).squeeze(dim=1)\n",
    "        batch_y = batch_y.float()\n",
    "\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss_train += loss.item()\n",
    "\n",
    "        predicted_train = (outputs > 0.5).float()\n",
    "        total_correct_train += (predicted_train == batch_y).sum().item()\n",
    "        total_samples_train += batch_y.size(0)\n",
    "\n",
    "    # Calculate average training loss for the epoch\n",
    "    average_loss_train = total_loss_train / len(train_loader)\n",
    "    accuracy_train = total_correct_train / total_samples_train\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    total_correct_val = 0\n",
    "    total_samples_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X_val, batch_y_val in val_loader:\n",
    "            outputs_val = model(batch_X_val).squeeze(dim=1)\n",
    "            batch_y_val = batch_y_val.float()\n",
    "\n",
    "            predicted_val = (outputs_val > 0.5).float()\n",
    "            total_correct_val += (predicted_val == batch_y_val).sum().item()\n",
    "            total_samples_val += batch_y_val.size(0)\n",
    "\n",
    "    accuracy_val = total_correct_val / total_samples_val\n",
    "\n",
    "    # Print metrics for the epoch\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Average Loss: {average_loss_train}, '\n",
    "          f'Training Accuracy: {accuracy_train * 100:.2f}%, '\n",
    "          f'Validation Accuracy: {accuracy_val * 100:.2f}%')\n",
    "\n",
    "    # Adjust learning rate using scheduler\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\DELL\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\DELL\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\DELL\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\DELL\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:189: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Model: \"modified_vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential (Sequential)     (None, 7, 7, 512)         14718720  \n",
      "                                                                 \n",
      " global_average_pooling2d (  multiple                  0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " sequential_1 (Sequential)   (None, 2)                 18890754  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33609474 (128.21 MB)\n",
      "Trainable params: 33609474 (128.21 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 72\u001b[0m\n\u001b[0;32m     69\u001b[0m TrainL \u001b[38;5;241m=\u001b[39m npz_file[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Reshape and resize the input data\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m X_train_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mcustom_resize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTrainF\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m y_train_tensor \u001b[38;5;241m=\u001b[39m TrainL\n\u001b[0;32m     74\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices((X_train_tensor, y_train_tensor))\u001b[38;5;241m.\u001b[39mshuffle(\u001b[38;5;241m10000\u001b[39m)\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 53\u001b[0m, in \u001b[0;36mcustom_resize\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcustom_resize\u001b[39m(data):\n\u001b[1;32m---> 53\u001b[0m     resized_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m))\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m]):\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "\n",
    "class ModifiedVGG16(tf.keras.Model):\n",
    "    def __init__(self, num_classes=2, input_channels=10):\n",
    "        super(ModifiedVGG16, self).__init__()\n",
    "        self.features = models.Sequential([\n",
    "            layers.Conv2D(64, kernel_size=3, padding='same', activation='relu', input_shape=(224, 224, input_channels)),\n",
    "            layers.Conv2D(64, kernel_size=3, padding='same', activation='relu'),\n",
    "            layers.MaxPooling2D(pool_size=2, strides=2),\n",
    "\n",
    "            layers.Conv2D(128, kernel_size=3, padding='same', activation='relu'),\n",
    "            layers.Conv2D(128, kernel_size=3, padding='same', activation='relu'),\n",
    "            layers.MaxPooling2D(pool_size=2, strides=2),\n",
    "\n",
    "            layers.Conv2D(256, kernel_size=3, padding='same', activation='relu'),\n",
    "            layers.Conv2D(256, kernel_size=3, padding='same', activation='relu'),\n",
    "            layers.Conv2D(256, kernel_size=3, padding='same', activation='relu'),\n",
    "            layers.MaxPooling2D(pool_size=2, strides=2),\n",
    "\n",
    "            layers.Conv2D(512, kernel_size=3, padding='same', activation='relu'),\n",
    "            layers.Conv2D(512, kernel_size=3, padding='same', activation='relu'),\n",
    "            layers.Conv2D(512, kernel_size=3, padding='same', activation='relu'),\n",
    "            layers.MaxPooling2D(pool_size=2, strides=2),\n",
    "\n",
    "            layers.Conv2D(512, kernel_size=3, padding='same', activation='relu'),\n",
    "            layers.Conv2D(512, kernel_size=3, padding='same', activation='relu'),\n",
    "            layers.Conv2D(512, kernel_size=3, padding='same', activation='relu'),\n",
    "            layers.MaxPooling2D(pool_size=2, strides=2)\n",
    "        ])\n",
    "        self.avgpool = layers.GlobalAveragePooling2D()\n",
    "        self.classifier = models.Sequential([\n",
    "            layers.Dense(4096, activation='relu'),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(4096, activation='relu'),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(num_classes)\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Input shape\n",
    "input_shape = (20, 80, 636228)  # Assuming you have spectrogram data with 20 channels\n",
    "\n",
    "# Resize the spectrogram to a size compatible with VGG16 (e.g., 224x224)\n",
    "def custom_resize(data):\n",
    "    resized_data = np.zeros((data.shape[0], 224, 224, data.shape[3]))\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[3]):\n",
    "            resized_data[i, :, :, j] = resize(data[i, :, :, j], (224, 224))\n",
    "    return resized_data\n",
    "\n",
    "# Create the TensorFlow model\n",
    "model = ModifiedVGG16(num_classes=2, input_channels=10)\n",
    "model.build((None, 224, 224, 10))  # Build the model with a specific input shape\n",
    "print(model.summary())\n",
    "\n",
    "file_path = 'Padded_Training.npz'\n",
    "npz_file = np.load(file_path)\n",
    "\n",
    "# Access individual arrays\n",
    "TrainF = npz_file['features']\n",
    "TrainL = npz_file['labels']\n",
    "\n",
    "# Reshape and resize the input data\n",
    "X_train_tensor = custom_resize(TrainF)\n",
    "y_train_tensor = TrainL\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_tensor, y_train_tensor)).shuffle(10000).batch(10)\n",
    "\n",
    "# Validation data\n",
    "val_file_path = 'Padded_Testing.npz'\n",
    "val_npz_file = np.load(val_file_path)\n",
    "ValF = val_npz_file['features']\n",
    "ValL = val_npz_file['labels']\n",
    "X_val_tensor = custom_resize(ValF)\n",
    "y_val_tensor = ValL\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val_tensor, y_val_tensor)).batch(5)\n",
    "\n",
    "# Define the criterion and optimizer\n",
    "criterion = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Training and Validation loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training loop\n",
    "    total_correct_train = 0\n",
    "    total_samples_train = 0\n",
    "\n",
    "    for batch_X, batch_y in train_dataset:\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(batch_y, outputs)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        predicted_train = tf.argmax(outputs, axis=1)\n",
    "        total_correct_train += tf.reduce_sum(tf.cast(tf.equal(predicted_train, batch_y), dtype=tf.float32)).numpy()\n",
    "        total_samples_train += batch_y.shape[0]\n",
    "\n",
    "    accuracy_train = total_correct_train / total_samples_train\n",
    "\n",
    "    # Validation loop\n",
    "    total_correct_val = 0\n",
    "    total_samples_val = 0\n",
    "\n",
    "    for batch_X_val, batch_y_val in val_dataset:\n",
    "        outputs_val = model(batch_X_val)\n",
    "        predicted_val = tf.argmax(outputs_val, axis=1)\n",
    "        total_correct_val += tf.reduce_sum(tf.cast(tf.equal(predicted_val, batch_y_val), dtype=tf.float32)).numpy()\n",
    "        total_samples_val += batch_y_val.shape[0]\n",
    "\n",
    "    accuracy_val = total_correct_val / total_samples_val\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.numpy()}, '\n",
    "          f'Training Accuracy: {accuracy_train * 100:.2f}%, '\n",
    "          f'Validation Accuracy: {accuracy_val * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 68.18%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "svm_model = SVC(kernel='rbf', C=4)\n",
    "\n",
    "\n",
    "svm_model.fit(TrainF, TrainL)\n",
    "\n",
    "y_pred = svm_model.predict(ValF)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(ValL, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "nb_model = GaussianNB()\n",
    "\n",
    "nb_model.fit(TrainF, TrainL)\n",
    "\n",
    "y_pred = nb_model.predict(ValF)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(ValL, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 54.55%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming you already have TrainF, TrainL, ValF, and ValL\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=50)\n",
    "\n",
    "# Train the model on the training data\n",
    "rf_model.fit(TrainF, TrainL)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_rf = rf_model.predict(ValF)\n",
    "\n",
    "# Evaluate the accuracy of the Random Forest model\n",
    "accuracy_rf = accuracy_score(ValL, y_pred_rf)\n",
    "print(f\"Random Forest Accuracy: {accuracy_rf * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
