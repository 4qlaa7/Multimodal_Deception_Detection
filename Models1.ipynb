{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUModel(\n",
      "  (gru1): LSTM(636228, 250, batch_first=True)\n",
      "  (fc): Linear(in_features=250, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Epoch [1/10], Average Loss: 0.7154402919113636, Training Accuracy: 51.25%, Validation Accuracy: 36.36%\n",
      "Epoch [2/10], Average Loss: 0.7033217065036297, Training Accuracy: 47.50%, Validation Accuracy: 45.45%\n",
      "Epoch [3/10], Average Loss: 0.696284182369709, Training Accuracy: 48.75%, Validation Accuracy: 45.45%\n",
      "Epoch [4/10], Average Loss: 0.6923133693635464, Training Accuracy: 50.00%, Validation Accuracy: 45.45%\n",
      "Epoch [5/10], Average Loss: 0.689653780311346, Training Accuracy: 50.00%, Validation Accuracy: 45.45%\n",
      "Epoch [6/10], Average Loss: 0.6891142316162586, Training Accuracy: 50.00%, Validation Accuracy: 45.45%\n",
      "Epoch [7/10], Average Loss: 0.6927569024264812, Training Accuracy: 50.00%, Validation Accuracy: 45.45%\n",
      "Epoch [8/10], Average Loss: 0.6898260712623596, Training Accuracy: 50.00%, Validation Accuracy: 40.91%\n",
      "Epoch [9/10], Average Loss: 0.6914833299815655, Training Accuracy: 50.00%, Validation Accuracy: 40.91%\n",
      "Epoch [10/10], Average Loss: 0.6901916302740574, Training Accuracy: 50.00%, Validation Accuracy: 40.91%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru1 = nn.LSTM(input_size=input_size, hidden_size=250, batch_first=True, bidirectional=False)\n",
    "        self.fc = nn.Linear(250, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.gru1(x)\n",
    "        \n",
    "        if len(x.shape) == 3:\n",
    "            x = self.fc(x[:, -1, :])\n",
    "        elif len(x.shape) == 2:\n",
    "            x = self.fc(x.unsqueeze(1)).squeeze(1)\n",
    "        else:\n",
    "            raise ValueError(\"Input tensor must have either two or three dimensions.\")\n",
    "        \n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Function to initialize model weights\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "# Input shape\n",
    "input_shape = (80, 636228, 1)\n",
    "\n",
    "# Create the PyTorch model with weight initialization\n",
    "model = GRUModel(input_shape[1])\n",
    "model.apply(init_weights)\n",
    "print(model)\n",
    "\n",
    "file_path = 'Padded_Training.npz'\n",
    "npz_file = np.load(file_path)\n",
    "\n",
    "# Access individual arrays\n",
    "TrainF = npz_file['features']\n",
    "TrainL = npz_file['labels']\n",
    "\n",
    "X_train_tensor = torch.tensor(TrainF, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(TrainL, dtype=torch.float32)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
    "\n",
    "# Validation data\n",
    "val_file_path = 'Padded_Testing.npz'\n",
    "val_npz_file = np.load(val_file_path)\n",
    "ValF = val_npz_file['features']\n",
    "ValL = val_npz_file['labels']\n",
    "X_val_tensor = torch.tensor(ValF, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(ValL, dtype=torch.float32)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=5, shuffle=False)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# Training, Validation, and Testing loops\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    total_loss_train = 0\n",
    "    total_correct_train = 0\n",
    "    total_samples_train = 0\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        outputs = model(batch_X).squeeze(dim=1)\n",
    "        batch_y = batch_y.float()\n",
    "\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss_train += loss.item()\n",
    "\n",
    "        predicted_train = (outputs > 0.5).float()\n",
    "        total_correct_train += (predicted_train == batch_y).sum().item()\n",
    "        total_samples_train += batch_y.size(0)\n",
    "\n",
    "    # Calculate average training loss for the epoch\n",
    "    average_loss_train = total_loss_train / len(train_loader)\n",
    "    accuracy_train = total_correct_train / total_samples_train\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    total_correct_val = 0\n",
    "    total_samples_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X_val, batch_y_val in val_loader:\n",
    "            outputs_val = model(batch_X_val).squeeze(dim=1)\n",
    "            batch_y_val = batch_y_val.float()\n",
    "\n",
    "            predicted_val = (outputs_val > 0.5).float()\n",
    "            total_correct_val += (predicted_val == batch_y_val).sum().item()\n",
    "            total_samples_val += batch_y_val.size(0)\n",
    "\n",
    "    accuracy_val = total_correct_val / total_samples_val\n",
    "\n",
    "    # Print metrics for the epoch\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Average Loss: {average_loss_train}, '\n",
    "          f'Training Accuracy: {accuracy_train * 100:.2f}%, '\n",
    "          f'Validation Accuracy: {accuracy_val * 100:.2f}%')\n",
    "\n",
    "    # Adjust learning rate using scheduler\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GRU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUModel(\n",
      "  (gru1): GRU(636228, 250, batch_first=True)\n",
      "  (fc): Linear(in_features=250, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Epoch [1/10], Loss: 0.7357641458511353, Training Accuracy: 45.00%, Validation Accuracy: 222.73%\n",
      "Epoch [2/10], Loss: 0.8199621438980103, Training Accuracy: 50.00%, Validation Accuracy: 222.73%\n",
      "Epoch [3/10], Loss: 0.6849316954612732, Training Accuracy: 50.00%, Validation Accuracy: 222.73%\n",
      "Epoch [4/10], Loss: 0.7496523261070251, Training Accuracy: 50.00%, Validation Accuracy: 222.73%\n",
      "Epoch [5/10], Loss: 0.6766282320022583, Training Accuracy: 50.00%, Validation Accuracy: 222.73%\n",
      "Epoch [6/10], Loss: 0.6851674318313599, Training Accuracy: 50.00%, Validation Accuracy: 222.73%\n",
      "Epoch [7/10], Loss: 0.6269125938415527, Training Accuracy: 50.00%, Validation Accuracy: 222.73%\n",
      "Epoch [8/10], Loss: 0.7277967929840088, Training Accuracy: 50.00%, Validation Accuracy: 222.73%\n",
      "Epoch [9/10], Loss: 0.7023771405220032, Training Accuracy: 50.00%, Validation Accuracy: 222.73%\n",
      "Epoch [10/10], Loss: 0.7348083853721619, Training Accuracy: 50.00%, Validation Accuracy: 222.73%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru1 = nn.GRU(input_size=input_size, hidden_size=250, batch_first=True, bidirectional=False)\n",
    "        self.fc = nn.Linear(250, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.gru1(x)\n",
    "        \n",
    "        if len(x.shape) == 3:\n",
    "            x = self.fc(x[:, -1, :])  # If the input has three dimensions, get the last output\n",
    "        elif len(x.shape) == 2:\n",
    "            x = self.fc(x.unsqueeze(1)).squeeze(1)  # If the input has two dimensions, add and squeeze a dimension\n",
    "        else:\n",
    "            raise ValueError(\"Input tensor must have either two or three dimensions.\")\n",
    "        \n",
    "        x = self.sigmoid(x)  # Adjust to have the same size as the target\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Input shape\n",
    "input_shape = (80, 636228, 1)\n",
    "\n",
    "# Create the PyTorch model\n",
    "model = GRUModel(input_shape[1])\n",
    "print(model)\n",
    "\n",
    "file_path = 'Padded_Training.npz'\n",
    "npz_file = np.load(file_path)\n",
    "\n",
    "# Access individual arrays\n",
    "TrainF = npz_file['features']\n",
    "TrainL = npz_file['labels']\n",
    "\n",
    "X_train_tensor = torch.tensor(TrainF, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(TrainL, dtype=torch.float32)  # Change dtype to float32 for binary classification\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
    "\n",
    "# Validation data\n",
    "val_file_path = 'Padded_Testing.npz'\n",
    "val_npz_file = np.load(val_file_path)\n",
    "ValF = val_npz_file['features']\n",
    "ValL = val_npz_file['labels']\n",
    "X_val_tensor = torch.tensor(ValF, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(ValL, dtype=torch.float32)  # Change dtype to float32 for binary classification\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=5, shuffle=False) \n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # Change loss function for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training, Validation, and Testing loops\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    total_correct_train = 0\n",
    "    total_samples_train = 0\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        outputs = model(batch_X).squeeze(dim=1)\n",
    "\n",
    "        # Ensure the target tensor is in the range [0, 1]\n",
    "        batch_y = batch_y.float()  # Convert to float\n",
    "        #batch_y = batch_y.unsqueeze(1)  # Ensure it has the correct shape\n",
    "\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        predicted_train = (outputs > 0.5).float()\n",
    "        total_correct_train += (predicted_train == batch_y).sum().item()\n",
    "        total_samples_train += batch_y.size(0)\n",
    "\n",
    "    accuracy_train = total_correct_train / total_samples_train\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    total_correct_val = 0\n",
    "    total_samples_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X_val, batch_y_val in val_loader:\n",
    "            outputs_val = model(batch_X_val).squeeze(dim=1)\n",
    "\n",
    "            # Ensure the target tensor is in the range [0, 1]\n",
    "            batch_y_val = batch_y_val.float()  # Convert to float\n",
    "            batch_y_val = batch_y_val.unsqueeze(1)  # Ensure it has the correct shape\n",
    "\n",
    "            predicted_val = (outputs_val > 0.5).float()\n",
    "            total_correct_val += (predicted_val == batch_y_val).sum().item()\n",
    "            total_samples_val += batch_y_val.size(0)\n",
    "\n",
    "    accuracy_val = total_correct_val / total_samples_val\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}, '\n",
    "          f'Training Accuracy: {accuracy_train * 100:.2f}%, '\n",
    "          f'Validation Accuracy: {accuracy_val * 100:.2f}%')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUModel(\n",
      "  (gru1): GRU(636228, 250, batch_first=True)\n",
      "  (fc): Linear(in_features=250, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Epoch [1/10], Average Loss: 0.7094924040138721, Training Accuracy: 48.75%, Validation Accuracy: 45.45%\n",
      "Epoch [2/10], Average Loss: 0.7028701528906822, Training Accuracy: 48.75%, Validation Accuracy: 50.00%\n",
      "Epoch [3/10], Average Loss: 0.6911830827593803, Training Accuracy: 51.25%, Validation Accuracy: 54.55%\n",
      "Epoch [4/10], Average Loss: 0.6893772594630718, Training Accuracy: 52.50%, Validation Accuracy: 54.55%\n",
      "Epoch [5/10], Average Loss: 0.70161347463727, Training Accuracy: 48.75%, Validation Accuracy: 54.55%\n",
      "Epoch [6/10], Average Loss: 0.6885979510843754, Training Accuracy: 50.00%, Validation Accuracy: 54.55%\n",
      "Epoch [7/10], Average Loss: 0.6973218694329262, Training Accuracy: 50.00%, Validation Accuracy: 54.55%\n",
      "Epoch [8/10], Average Loss: 0.6945449151098728, Training Accuracy: 51.25%, Validation Accuracy: 54.55%\n",
      "Epoch [9/10], Average Loss: 0.6900071986019611, Training Accuracy: 51.25%, Validation Accuracy: 54.55%\n",
      "Epoch [10/10], Average Loss: 0.692377369850874, Training Accuracy: 50.00%, Validation Accuracy: 54.55%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru1 = nn.GRU(input_size=input_size, hidden_size=250, batch_first=True, bidirectional=False)\n",
    "        self.fc = nn.Linear(250, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.gru1(x)\n",
    "        \n",
    "        if len(x.shape) == 3:\n",
    "            x = self.fc(x[:, -1, :])\n",
    "        elif len(x.shape) == 2:\n",
    "            x = self.fc(x.unsqueeze(1)).squeeze(1)\n",
    "        else:\n",
    "            raise ValueError(\"Input tensor must have either two or three dimensions.\")\n",
    "        \n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Function to initialize model weights\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "# Input shape\n",
    "input_shape = (80, 636228, 1)\n",
    "\n",
    "# Create the PyTorch model with weight initialization\n",
    "model = GRUModel(input_shape[1])\n",
    "model.apply(init_weights)\n",
    "print(model)\n",
    "\n",
    "file_path = 'Padded_Training.npz'\n",
    "npz_file = np.load(file_path)\n",
    "\n",
    "# Access individual arrays\n",
    "TrainF = npz_file['features']\n",
    "TrainL = npz_file['labels']\n",
    "\n",
    "X_train_tensor = torch.tensor(TrainF, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(TrainL, dtype=torch.float32)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
    "\n",
    "# Validation data\n",
    "val_file_path = 'Padded_Testing.npz'\n",
    "val_npz_file = np.load(val_file_path)\n",
    "ValF = val_npz_file['features']\n",
    "ValL = val_npz_file['labels']\n",
    "X_val_tensor = torch.tensor(ValF, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(ValL, dtype=torch.float32)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=5, shuffle=False)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# Training, Validation, and Testing loops\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    total_loss_train = 0\n",
    "    total_correct_train = 0\n",
    "    total_samples_train = 0\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        outputs = model(batch_X).squeeze(dim=1)\n",
    "        batch_y = batch_y.float()\n",
    "\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss_train += loss.item()\n",
    "\n",
    "        predicted_train = (outputs > 0.5).float()\n",
    "        total_correct_train += (predicted_train == batch_y).sum().item()\n",
    "        total_samples_train += batch_y.size(0)\n",
    "\n",
    "    # Calculate average training loss for the epoch\n",
    "    average_loss_train = total_loss_train / len(train_loader)\n",
    "    accuracy_train = total_correct_train / total_samples_train\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    total_correct_val = 0\n",
    "    total_samples_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X_val, batch_y_val in val_loader:\n",
    "            outputs_val = model(batch_X_val).squeeze(dim=1)\n",
    "            batch_y_val = batch_y_val.float()\n",
    "\n",
    "            predicted_val = (outputs_val > 0.5).float()\n",
    "            total_correct_val += (predicted_val == batch_y_val).sum().item()\n",
    "            total_samples_val += batch_y_val.size(0)\n",
    "\n",
    "    accuracy_val = total_correct_val / total_samples_val\n",
    "\n",
    "    # Print metrics for the epoch\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Average Loss: {average_loss_train}, '\n",
    "          f'Training Accuracy: {accuracy_train * 100:.2f}%, '\n",
    "          f'Validation Accuracy: {accuracy_val * 100:.2f}%')\n",
    "\n",
    "    # Adjust learning rate using scheduler\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMModel(\n",
      "  (lstm1): LSTM(636228, 250, batch_first=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Epoch [1/10], Loss: 5.507979393005371, Training Accuracy: 0.00%, Validation Accuracy: 0.00%\n",
      "Epoch [2/10], Loss: 5.508194923400879, Training Accuracy: 0.00%, Validation Accuracy: 0.00%\n",
      "Epoch [3/10], Loss: 5.506131172180176, Training Accuracy: 0.00%, Validation Accuracy: 0.00%\n",
      "Epoch [4/10], Loss: 5.509807109832764, Training Accuracy: 0.00%, Validation Accuracy: 0.00%\n",
      "Epoch [5/10], Loss: 5.506494998931885, Training Accuracy: 0.00%, Validation Accuracy: 0.00%\n",
      "Epoch [6/10], Loss: 5.48881721496582, Training Accuracy: 0.00%, Validation Accuracy: 0.00%\n",
      "Epoch [7/10], Loss: 5.486522197723389, Training Accuracy: 0.00%, Validation Accuracy: 0.00%\n",
      "Epoch [8/10], Loss: 5.494141101837158, Training Accuracy: 0.00%, Validation Accuracy: 0.00%\n",
      "Epoch [9/10], Loss: 5.5052571296691895, Training Accuracy: 2.50%, Validation Accuracy: 0.00%\n",
      "Epoch [10/10], Loss: 5.419958114624023, Training Accuracy: 5.00%, Validation Accuracy: 9.09%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=250, batch_first=True, bidirectional=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Input shape\n",
    "input_shape = (80, 636228, 1)\n",
    "num_classes = 2\n",
    "\n",
    "# Create the PyTorch model\n",
    "model = LSTMModel(input_shape[1], 2)\n",
    "print(model)\n",
    "\n",
    "file_path = 'Padded_Training.npz'\n",
    "npz_file = np.load(file_path)\n",
    "\n",
    "# Access individual arrays\n",
    "TrainF = npz_file['features']\n",
    "TrainL = npz_file['labels']\n",
    "\n",
    "X_train_tensor = torch.tensor(TrainF, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(TrainL, dtype=torch.long)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "\n",
    "# Validation data\n",
    "val_file_path = 'Padded_Testing.npz'\n",
    "val_npz_file = np.load(val_file_path)\n",
    "ValF = val_npz_file['features']\n",
    "ValL = val_npz_file['labels']\n",
    "X_val_tensor = torch.tensor(ValF, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(ValL, dtype=torch.long)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training, Validation, and Testing loops\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    total_correct_train = 0\n",
    "    total_samples_train = 0\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted_train = torch.max(outputs, 1)\n",
    "        total_correct_train += (predicted_train == batch_y).sum().item()\n",
    "        total_samples_train += batch_y.size(0)\n",
    "\n",
    "    accuracy_train = total_correct_train / total_samples_train\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    total_correct_val = 0\n",
    "    total_samples_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X_val, batch_y_val in val_loader:\n",
    "            outputs_val = model(batch_X_val)\n",
    "            _, predicted_val = torch.max(outputs_val, 1)\n",
    "            total_correct_val += (predicted_val == batch_y_val).sum().item()\n",
    "            total_samples_val += batch_y_val.size(0)\n",
    "\n",
    "    accuracy_val = total_correct_val / total_samples_val\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}, '\n",
    "          f'Training Accuracy: {accuracy_train * 100:.2f}%, '\n",
    "          f'Validation Accuracy: {accuracy_val * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 68.18%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "svm_model = SVC(kernel='rbf', C=4)\n",
    "\n",
    "# Train the model on the training data\n",
    "svm_model.fit(TrainF, TrainL)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = svm_model.predict(ValF)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(ValL, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "nb_model = GaussianNB()\n",
    "\n",
    "nb_model.fit(TrainF, TrainL)\n",
    "\n",
    "y_pred = nb_model.predict(ValF)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(ValL, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 54.55%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming you already have TrainF, TrainL, ValF, and ValL\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=50)\n",
    "\n",
    "# Train the model on the training data\n",
    "rf_model.fit(TrainF, TrainL)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_rf = rf_model.predict(ValF)\n",
    "\n",
    "# Evaluate the accuracy of the Random Forest model\n",
    "accuracy_rf = accuracy_score(ValL, y_pred_rf)\n",
    "print(f\"Random Forest Accuracy: {accuracy_rf * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
